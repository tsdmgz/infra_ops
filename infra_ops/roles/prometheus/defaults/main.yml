---
# defaults file for prometheus

__prometheus_config_dir: /etc/prometheus

prometheus_pod_name: prometheus
prometheus_network_ipv6: true
prometheus_image_url: docker.io/prom/prometheus:latest
prometheus_internal_listen_port: 9090
prometheus_external_listen_port: "{{ prometheus_internal_listen_port }}"
prometheus_volume_list:
  - "{{ prometheus_pod_name }}-prometheus_vol-data:/etc/prometheus/"
prometheus_ports_list:
  - "{{ prometheus_internal_listen_port }}:{{ prometheus_external_listen_port }}"
prometheus_health_check: "wget -S -T 2 -O - http://localhost:{{ prometheus_internal_listen_port }}"
prometheus_launch_command_list:
  - "--storage.tsdb.retention.time=180d"
  - "--storage.tsdb.path=/etc/prometheus/data"
  - "--config.file={{ __prometheus_config_dir }}/prometheus.yml"
  - "--web.enable-lifecycle"
prometheus_file_serole: object_r
prometheus_file_seuser: system_u
prometheus_file_setype: container_file_t

prometheus_config_global:
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 15s

prometheus_config_alertmanager_config: []

prometheus_config_scrape_targets: {}
#  node:
#    - targets:
#        - localhost:9100
#      labels:
#        env: test
#
prometheus_config_scrape_config:
  - job_name: "node"
    file_sd_configs:
      - files:
        - "{{ __prometheus_config_dir }}/file_sd/node.yml"

prometheus_config_alert_rules_files:
  - prometheus/rules/*.rules

prometheus_config_alert_relabel_configs: []

prometheus_config_alert_rules:
  - alert: Watchdog
    expr: vector(1)
    for: 10m
    labels:
      severity: warning
    annotations:
      description: "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty."
      summary: 'Ensure entire alerting pipeline is functional'
  - alert: InstanceDown
    expr: 'up{job="node"} == 0'
    for: 5m
    labels:
      severity: critical
    annotations:
      description: '{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.{% endraw %}'
      summary: '{% raw %}Instance {{ $labels.instance }} down{% endraw %}'
